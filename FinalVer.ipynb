{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, world\n"
     ]
    }
   ],
   "source": [
    "print('hello, world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will go through the steps required for all five preposed versions of RAGSync. The versions included within this file are:\n",
    "\n",
    "* V1: Self Query Retriever (Most Data intensive)(Chroma Dependent)(Buggy as the SelfQueryRetriever is known to having issues)\n",
    "* V2: Ensemble Retriever (FAISS Dependent)\n",
    "* V3: Ensemble Retriever + Multi Query Contruction with RagFusion ReRanking (Best one; Most Compute intensive)(FAISS Dependent)\n",
    "* V4: Straight Rag (Most simple)(FAISS Dependent)\n",
    "* V5: Straight Rag with Single Query Contruction (Middle Ground between compute and complexity)(FAISS Dependent)\n",
    "\n",
    "Futher, theory and the application of said versions will be explained within their sections as well.\n",
    "\n",
    "The project's running is split into three parts: \n",
    "* Indexing: The process of storing the documents into vectors. \n",
    "* Retrieval: The process of getting the relevent documents\n",
    "* Generation: The process of generating the answers based on the user query. \n",
    "\n",
    "Let us move to the static setup of system in order get the project working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defaults:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project gives you options for a lot of choices that you would have to take for a RAG Based LLM Application. These choices have a default option and alternate options and you are free to select methods outside of the provided options. Make your choices wisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an new python env and then install the requriments.txt using pip to setup the env. \n",
    "\n",
    "By Default, using python 3.8.19 for testing the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing of Documents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of the current working directory:\n",
    "\n",
    "```\n",
    "working_directory/\n",
    "│\n",
    "├── data/\n",
    "│   ├── RawData/\n",
    "│   └── SupportingDoc/\n",
    "├── FinalVer.ipynb\n",
    "├── Learning_Documentation.md\n",
    "├── README.md\n",
    "└── requirements.txt\n",
    "```\n",
    "\n",
    "**Drop off your Main Rag files** that you want to fetch the answers from in the **RawData** folder, and **drop off your supporting documents** that the model uses make sense of the contents from your main rag file in the **SupportingDoc** folder.\n",
    "\n",
    "**NOTE: ONLY .TXT FILES ARE SUPPORTED**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of Secret Keys:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the ```Readme.md``` file in order create the key for AWS BedRock along with Llama3 family of models, along with GROQ Key if your are changing the default to GROQ.\n",
    "\n",
    "Create an ```.env``` file within the working directory and the store the AWS Keys under the names of: \n",
    "* AWS_ACCESS_KEY_ID\n",
    "* AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "And if in case of GROQ, store it under the name of: \n",
    "* GROQ_API_KEY\n",
    "\n",
    "No other keys or services are requried for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of Embedding Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding function is what turns textual data into numbers, then later on into vectors. The choice of embedding function is very important. You can choose any one of the below two for your usecase: \n",
    "* BGE-Base-en-v1.5: Larger in size, more computation, better results for longer context. \n",
    "* BGE-Small-en-v1.5: Smaller in size, less computation, a bit worse than BGE-Base-en-v1.5 in longer context due to smaller embedding size. \n",
    "\n",
    "By default, BGE-Base-en-v1.5 is being used, but feel free to change it up to any embedding function within BGE or even any other function from Hugging Face.\n",
    "\n",
    "When running this code for the first time, it will download the files from Hugging Face, later on when recalling the embedding function, it will call the cached files.\n",
    "\n",
    "The below two blocks of code is for downloading and loading of BGE-Base-en-v1.5 and BGE-Small-en-v1.5. Run any one and leave out the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='BAAI/bge-base-en-v1.5' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': True} query_instruction='Represent this question for searching relevant passages: ' embed_instruction='' show_progress=True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    DEFAULT EMBEDDING FUNCTION OPTION: BGE-BASE-EN-v1.5\n",
    "    This block of code is to download and load up the embedding function\n",
    "\"\"\"\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs, show_progress=True\n",
    ")\n",
    "print(hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='BAAI/bge-small-en-v1.5' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': True} query_instruction='Represent this question for searching relevant passages: ' embed_instruction='' show_progress=True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    ALTERNATE EMBEDDING FUNCTION OPTION: BGE-SMALL-EN-v1.5\n",
    "    This block of code is to download and load up the embedding function\n",
    "\"\"\"\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs, show_progress=True\n",
    ")\n",
    "print(hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Vector Database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is the most time consuming part of the project, where you are required to create the vector database. You have two options for your vector Database: \n",
    "* ChromaDB: Slower but works with Self Query Retrieval\n",
    "* FAISS: Faster but doesn't support Self Query Retrieval\n",
    "\n",
    "By Default, FAISS is recommended unless you want to try out Self Query Retrieval. \n",
    "\n",
    "The below two blocks of code is for the creation of FAISS and CHROMADB respectively. Run anyone and leave out the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read and split! (RD)\n",
      "Directory created: c:\\Users\\yaesh\\OneDrive\\Desktop\\College\\Projects\\RAGSync\\Data\\FAISS_data  (RD)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04a849803af4786b80eac8d1e9bbf3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process done for RawData\n",
      "Files read and split! (SD)\n",
      "Directory created: c:\\Users\\yaesh\\OneDrive\\Desktop\\College\\Projects\\RAGSync\\Data\\FAISS_SDdata  (SD)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c0c203ba0348d8a8e90ab984a1b36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process done for SupportingDoc\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    DEFAULT VECTOR DATABASE OPTION: FAISS\n",
    "    This block of code is to create the folder and then create the vector database within the folder. \n",
    "\"\"\"\n",
    "#The below is to read the documents from RawData and spliting them into chunks.\n",
    "import os\n",
    "import shutil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "loader = DirectoryLoader(\n",
    "    \"Data\\RawData\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, is_separator_regex=False, separators=\"\\n\")\n",
    "splits = text_splitter.split_documents(documents) #Splits the files into chunks.\n",
    "print(\"Files read and split! (RD)\")\n",
    "\n",
    "#The below is to create the folder for the vector database for RawData\n",
    "#In the case of the folder already existing, it will delete the create an new folder.\n",
    "FAISS_PATH = os.path.join(os.getcwd(), \"Data\\FAISS_data\")\n",
    "if not os.path.exists(FAISS_PATH):\n",
    "    os.makedirs(FAISS_PATH, exist_ok=True) #Creates an new folder\n",
    "else:\n",
    "    # Delete all files and folders in FAISS_PATH\n",
    "    shutil.rmtree(FAISS_PATH)\n",
    "    os.makedirs(FAISS_PATH, exist_ok=True)\n",
    "print(f\"Directory created: {FAISS_PATH}  (RD)\")\n",
    "\n",
    "#The below is the create the vector database for RawData. This will take a while, depending on your system's specs.\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=hf_embeddings) #This is to create the database from scratch.\n",
    "vectorstore.save_local(FAISS_PATH)\n",
    "print(\"Process done for RawData\")\n",
    "\n",
    "\n",
    "#The below does the same as above, but for the supporting documents. \n",
    "SDloader = DirectoryLoader(\n",
    "    \"Data\\SupportingDoc\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "SDdocuments = SDloader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "SDsplits = text_splitter.split_documents(SDdocuments)\n",
    "print(\"Files read and split! (SD)\")\n",
    "\n",
    "FAISS_SDPATH = os.path.join(os.getcwd(), \"Data\\FAISS_SDdata\")\n",
    "if not os.path.exists(FAISS_SDPATH):\n",
    "    os.makedirs(FAISS_SDPATH, exist_ok=True) #Creates an new folder\n",
    "else:\n",
    "    # Delete all files and folders in FAISS_PATH\n",
    "    shutil.rmtree(FAISS_SDPATH)\n",
    "    os.makedirs(FAISS_SDPATH, exist_ok=True)\n",
    "print(f\"Directory created: {FAISS_SDPATH}  (SD)\")\n",
    "\n",
    "#The below is the create the vector database for RawData. This will take a while, depending on your system's specs.\n",
    "SDvectorstore = FAISS.from_documents(documents=SDsplits, embedding=hf_embeddings) #This is to create the database from scratch.\n",
    "SDvectorstore.save_local(FAISS_SDPATH)\n",
    "print(\"Process done for SupportingDoc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    ALTERNATE VECTOR DATABASE OPTION: CHROMA\n",
    "    This block of code is to create the folder and then create the vector database within the folder. \n",
    "    Since ChromaDB is being used for the Self Query Retrieval, we have to create and include the meta data.\n",
    "\"\"\"\n",
    "#The below is to read the documents from RawData and spliting them into chunks.\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from collections import Counter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#The below is the helper functions\n",
    "def extract_timestamp_and_tags(log_entry):\n",
    "    timestamp_pattern = r'^(\\w+ \\d{2} \\d{2}:\\d{2}:\\d{2})'\n",
    "    tags_pattern = r'\\[(.*?)\\]'\n",
    "    key_state_pattern = r'-> (\\w+)'\n",
    "\n",
    "    # Find the timestamp\n",
    "    match = re.search(timestamp_pattern, log_entry)\n",
    "    if match:\n",
    "        timestamp = match.group(1)\n",
    "        \n",
    "        # Find the tags\n",
    "        tags_match = re.findall(tags_pattern, log_entry)\n",
    "        \n",
    "        # Assign special variables\n",
    "        first_tag = tags_match.pop(0) if len(tags_match) > 0 else ''\n",
    "        thid = tags_match[0].strip('[ThId: ]') if len(tags_match) > 1 else ''\n",
    "        third_tag = tags_match[1].strip() if len(tags_match) > 2 else ''\n",
    "        \n",
    "        # Check for key state\n",
    "        key_state_match = re.search(key_state_pattern, log_entry)\n",
    "        key_state = key_state_match.group(1) if key_state_match else ''\n",
    "        \n",
    "        # Join remaining tags with spaces\n",
    "        other_tags = ' '.join(tags_match[2:]) if len(tags_match) >= 3 else ''\n",
    "        \n",
    "        return timestamp, first_tag, third_tag, other_tags, key_state, thid\n",
    "    \n",
    "    return None, None, None, None, None, None\n",
    "\n",
    "def extract_timestamps_and_tags_from_log_string(log_string):\n",
    "    log_entries = log_string.strip().split('\\n')\n",
    "    \n",
    "    entries = []\n",
    "    \n",
    "    for entry in log_entries:\n",
    "        timestamp, first_tag, third_tag, other_tags, key_state, thid = extract_timestamp_and_tags(entry)\n",
    "        if timestamp and first_tag and third_tag:\n",
    "            entries.append((timestamp, first_tag, third_tag, other_tags, key_state, thid))\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def include_MetaData(splits):\n",
    "    for i in range(len(splits)):\n",
    "        entries = extract_timestamps_and_tags_from_log_string(splits[i].page_content)\n",
    "        \n",
    "        # Combine all timestamps into one list\n",
    "        all_timestamps = [t[0] for t in entries]\n",
    "        start_time_stamp = all_timestamps[0]\n",
    "        end_time_stamp = all_timestamps[-1]\n",
    "        \n",
    "        # Extract unique values efficiently\n",
    "        logtypes = list(filter(None, Counter(t[1] for t in entries).keys()))\n",
    "        maintags = list(filter(None, Counter(t[2] for t in entries).keys()))\n",
    "        other_tags = list(filter(None, Counter(t[3] for t in entries).keys()))\n",
    "        key_states = list(filter(None, Counter(t[4] for t in entries).keys()))\n",
    "        thids = list(filter(None, Counter(t[5] for t in entries).keys()))\n",
    "        \n",
    "        splits[i].metadata[\"Start_Time_Stamp\"] = start_time_stamp\n",
    "        splits[i].metadata[\"End_Time_Stamp\"] = end_time_stamp\n",
    "        splits[i].metadata[\"Log_Type_Tag\"] = \" , \".join(i for i in logtypes)\n",
    "        splits[i].metadata[\"Main_Tag\"] = \" , \".join(i for i in maintags)\n",
    "        splits[i].metadata[\"Other_Tags\"] = \" , \".join(i for i in other_tags)\n",
    "        splits[i].metadata[\"Key_States\"] = \" , \".join(i for i in key_states)\n",
    "        splits[i].metadata['THID'] = \" , \".join(i for i in thids)\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"Data\\RawData\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, is_separator_regex=False, separators=\"\\n\")\n",
    "splits = text_splitter.split_documents(documents) #Splits the files into chunks.\n",
    "include_MetaData(splits)\n",
    "print(\"Files read and split! (RD)\")\n",
    "\n",
    "#The below is to create the folder for the vector database for RawData\n",
    "#In the case of the folder already existing, it will delete the create an new folder.\n",
    "CHROMA_PATH = os.path.join(os.getcwd(), \"Data\\CHROMA_data\")\n",
    "if not os.path.exists(CHROMA_PATH):\n",
    "    os.makedirs(CHROMA_PATH, exist_ok=True) #Creates an new folder\n",
    "else:\n",
    "    # Delete all files and folders in FAISS_PATH\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "print(f\"Directory created: {CHROMA_PATH}  (RD)\")\n",
    "\n",
    "#The below is the create the vector database for RawData. This will take a while, depending on your system's specs.\n",
    "#The below is to create the database from scratch.\n",
    "vectorstore_SD = Chroma.from_documents(documents=splits, embedding=hf_embeddings, persist_directory=CHROMA_PATH)\n",
    "print(\"Process done for RawData\")\n",
    "\n",
    "\n",
    "#The below does the same as above, but for the supporting documents. \n",
    "SDloader = DirectoryLoader(\n",
    "    \"Data\\SupportingDoc\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "SDdocuments = SDloader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "SDsplits = text_splitter.split_documents(SDdocuments)\n",
    "print(\"Files read and split! (SD)\")\n",
    "\n",
    "CHROMA_SDPATH = os.path.join(os.getcwd(), \"Data\\CHROMA_SDdata\")\n",
    "if not os.path.exists(CHROMA_SDPATH):\n",
    "    os.makedirs(CHROMA_SDPATH, exist_ok=True) #Creates an new folder\n",
    "else:\n",
    "    # Delete all files and folders in FAISS_PATH\n",
    "    shutil.rmtree(CHROMA_SDPATH)\n",
    "    os.makedirs(CHROMA_SDPATH, exist_ok=True)\n",
    "print(f\"Directory created: {CHROMA_SDPATH}  (SD)\")\n",
    "\n",
    "#The below is the create the vector database for RawData. This will take a while, depending on your system's specs.\n",
    "#The below is to create the database from scratch.\n",
    "vectorstore_SD = Chroma.from_documents(documents=SDsplits, embedding=hf_embeddings, persist_directory=CHROMA_SDPATH)\n",
    "print(\"Process done for SupportingDoc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipping to location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of you using the vector database you already have. Take the zip of the databases and then unzip them inside the Data Folder. \n",
    "\n",
    "The final directory structure should be: \n",
    "\n",
    "```\n",
    "working_directory/\n",
    "│\n",
    "├── data/\n",
    "│   ├── RawData/\n",
    "│   ├── SupportingDoc/\n",
    "│   ├── FAISS_data/ (VectorDatabase of the main files)\n",
    "│   └── FAISS_SDdata/ (VectorDatabase of the supporting files)\n",
    "├── FinalVer.ipynb\n",
    "├── Learning_Documentation.md\n",
    "├── README.md\n",
    "└── requirements.txt\n",
    "```\n",
    "\n",
    "If you wish to use ChromaDB, make sure that your database have meta data in them as that is required for Self Query Retriever and then change the names of the main file and supporting file folders for the ChromaDB Database as: ```CHROMA_data``` & ```CHROMA_SDdata```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main CodeBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup (Common For All Versions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is two services for you to choose from:\n",
    "* AWS BedRock \n",
    "* ChatQROQ\n",
    "\n",
    "By Default, AWS BedRock is being used for testing, but ChatQROQ can also be used. \n",
    "\n",
    "As of for the LLM, any model that you wish to use is fine. By Default, Llama3 8b and mistral.mixtral-8x7b-instruct-v0:1 is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    DEFAULT SERVICE OPTION: AWS BEDROCK\n",
    "    This block of code is to setup the LLM within AWS Bedrock\n",
    "\"\"\"\n",
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "aws_access_key = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "bedrock=boto3.client(service_name=\"bedrock-runtime\", region_name='ap-south-1')\n",
    "llm = ChatBedrock(model_id=\"meta.llama3-8b-instruct-v1:0\",client=bedrock, model_kwargs={\"temperature\": 0.1, \"top_p\": 0.9})\n",
    "# To Test the model\n",
    "# llm.invoke(\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    DEFAULT SERVICE OPTION: CHAT QROQ\n",
    "    This block of code is to setup the LLM within CHAT QROQ\n",
    "\"\"\"\n",
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)\n",
    "# To Test the model\n",
    "# llm.invoke(\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector DataBase and Retriever Loading (Common For All Versions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is two services for you to choose from:\n",
    "* FAISS \n",
    "* ChromaDB\n",
    "\n",
    "By Default, FAISS is being used for testing, but ChromaDB can also be used. \n",
    "\n",
    "As of for the embedding function, any model that you wish to use is fine. By Default, BGE-Base-en-v1.5 is being used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding function with Default and Alternate Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\miniconda3\\envs\\rag\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='BAAI/bge-base-en-v1.5' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': True} query_instruction='Represent this question for searching relevant passages: ' embed_instruction='' show_progress=True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    USE THE SAME EMBEDDING FUNCTION YOU USED WHILE CREATING VECTOR DATABASE.\n",
    "    This block of code is to load up the embedding function. \n",
    "    Both Alternate and default is mentioned here. Use any one and then comment out the other. \n",
    "\"\"\"\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "#The below is the BGE-Base-en-v1.5\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs, show_progress=True\n",
    ")\n",
    "print(hf_embeddings)\n",
    "\n",
    "\n",
    "# OR\n",
    "\n",
    "\n",
    "#The below is the BGE-Small-en-v1.5\n",
    "# model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "# model_kwargs = {\"device\": \"cpu\"}\n",
    "# encode_kwargs = {\"normalize_embeddings\": True}\n",
    "# hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "#     model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs, show_progress=True\n",
    "# )\n",
    "# print(hf_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default and Alternate Option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    This is to use FAISS to load the vectordb. \n",
    "    To use, Chroma, uncomment the Chroma code and comment out the FAISS code\n",
    "'''\n",
    "import os\n",
    "import shutil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "#Below is FAISS Code\n",
    "FAISS_PATH = os.path.join(os.getcwd(), \"Data\\FAISS_data\")\n",
    "vectorstore = FAISS.load_local(FAISS_PATH, hf_embeddings, allow_dangerous_deserialization = True)\n",
    "retriever = vectorstore.as_retriever()\n",
    "FAISS_SDPATH = os.path.join(os.getcwd(), \"Data\\FAISS_SDdata\")\n",
    "vectorstore_SD = FAISS.load_local(FAISS_SDPATH, hf_embeddings, allow_dangerous_deserialization = True)\n",
    "retriever_SD = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "# OR\n",
    "\n",
    "\n",
    "#Below is Chroma Code\n",
    "# CHROMA_PATH = os.path.join(os.getcwd(), \"Data\", \"CHROMA_data\")\n",
    "# vectorstore = Chroma(persist_directory=CHROMA_PATH, embedding_function=hf_embeddings)\n",
    "# retriever = vectorstore.as_retriever()\n",
    "# CHROMA_SDPATH = os.path.join(os.getcwd(), \"Data\", \"CHROMA_SDdata\")\n",
    "# vectorstore_SD = Chroma(persist_directory=CHROMA_SDPATH, embedding_function=hf_embeddings)\n",
    "# retriever_SD = vectorstore_SD.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Documents (Common For All Versions):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some versions of the project, the documents are required to be feed into the retrieval process. \n",
    "\n",
    "So, run this section for all versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader \n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\n",
    "    \"Data\\RawData\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,is_separator_regex=False, separators=\"\\n\")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"Data\\SupportingDoc\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200,is_separator_regex=False, separators=\"\\n\")\n",
    "SDsplits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 : Self Query Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"Time_Stamp\",\n",
    "        description=\"Start time of the log entry\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Log_Type_Tag\",\n",
    "        description=\"Type of log (INFO, WARN, ERROR), seperated by comma\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Main_Tag\",\n",
    "        description=\"Primary module or component responsible for the log, seperated by comma\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Other_Tags\",\n",
    "        description=\"Additional details about the log entry, seperated by comma\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"Key_States\",\n",
    "        description=\"Important provisioning states achieved during this log chunk, seperated by comma.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"THID\",\n",
    "        description=\"Unique identifier for the logging file, seperated by comma\",\n",
    "        type=\"string\",\n",
    "    )\n",
    "]\n",
    "\n",
    "document_content_description = \"\"\"Development log entries from IoT devices, including timestamps, log types, and various tags.\"\"\"\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    ")\n",
    "\n",
    "#To Test\n",
    "# Question = \"Enter your Question here\"\n",
    "# docs = retriever.invoke(Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Imagine you are a developer and give me answer to the question based on the context alone.\n",
    "Since infomation within the context might be not be common knowledge and be known to you, I have included another supporting document for you to understand the context even better:\n",
    "\n",
    "And, be very direct with your answers. Include futher infomation if needed \n",
    "\n",
    "question : {question}\n",
    "\n",
    "supporting document: {supporting_document}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retriever,\n",
    "     \"supporting_document\": retriever_SD | format_docs,\n",
    "     \"question\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Latest Log\"\n",
    "final_rag_chain.invoke(Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2 : Ensemble Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def log_and_retrieve(question):\n",
    "    print(f\"Query passed to retriever: {question}\")\n",
    "    return question\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "def findQuestion(text):\n",
    "    return re.findall(r'\"(.*?)\"', text)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 = '''\n",
    "Imagine you are my mentor and i was given the task of analyzing IoT Log data and i have many questions that i have to find answer for.\n",
    "I come to you with one of those many questions and you are going to help me figure out how to answer the questions using the documentation that i am giving you. \n",
    "\n",
    "question : {user_query}\n",
    "\n",
    "documentation: {log_documentation}\n",
    "'''\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(template1)\n",
    "\n",
    "final_rag_chain_QA = (\n",
    "    {\"user_query\": RunnablePassthrough(),\n",
    "     \"log_documentation\": retriever_SD | format_docs} \n",
    "    | prompt1\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#To Test\n",
    "# Question = \"Enter your Question here\"\n",
    "# print(final_rag_chain_QA.invoke(Question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template2 = '''\n",
    "My mentor had given me a way to answer my assignment question but i am not sure on how exactly to make sense of it. Could you read my mentor's advice and then boil it down to a single question with all of the technical terms so that i do key word search and find the answer easily?\n",
    "\n",
    "The advice: '{text}'\n",
    "\n",
    "note: please provide the finalized question with all the key technical terms enclosed with quotes\n",
    "'''\n",
    "\n",
    "prompt2 = ChatPromptTemplate.from_template(template2)\n",
    "\n",
    "final_rag_chain_QA2 = (\n",
    "    {\"text\": final_rag_chain_QA}\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | findQuestion\n",
    ")\n",
    "\n",
    "#To Test\n",
    "# Question = \"Enter your Question here\"\n",
    "print(final_rag_chain_QA2.invoke(Question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# RAG\n",
    "template = \"\"\"Imagine you are a developer and give me answer to the question based on the context alone.\n",
    "Since infomation within the context might be not be common knowledge and be known to you, I have included another supporting document for you to understand the context even better:\n",
    "\n",
    "And, be very direct with your answers. Include futher infomation if needed \n",
    "\n",
    "question : {question}\n",
    "\n",
    "supporting document: {supporting_document}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs,\n",
    "     \"supporting_document\": retriever_SD | format_docs,\n",
    "     \"question\": final_rag_chain_QA2 | log_and_retrieve} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Enter your Question here\"\n",
    "print(final_rag_chain.invoke(Question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3 : Ensemble Retriever + Multi Query Contruction with RagFusion ReRanking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load import dumps, loads\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_Tdocs(docs, k = 10):\n",
    "    return \"\\n\\n\".join(doc[0].page_content for doc in docs[0:k+1])\n",
    "\n",
    "def createQuestionsList(text):\n",
    "    return text.split(\"\\n\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def reciprocal_rank_fusion(results, k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, retriever], weights=[0.5, 0.5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions separated by newlines. Original question: {question}\n",
    "\n",
    "Since there are some infomation that should be added to the question in order to provide the best results to retrieve the documents from the vector database. \n",
    "You are supposed to include technical keywords to further help the document retrieval process.\n",
    "As the LLM which will analysis these questions and the retrieved documents have no prior knowledge on the documents nor the question, make the question technical\n",
    "The required documents are below: \n",
    "\n",
    "Required Documents: {context}\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# getMoreQuestions = ({\"context\": retriever_SD | format_docs, \"question\": RunnablePassthrough()} \n",
    "#                     | prompt_rag_fusion \n",
    "#                     | ChatBedrock(model_id=\"mistral.mixtral-8x7b-instruct-v0:1\",client=bedrock, model_kwargs={\"temperature\": 0.1, \"top_p\": 0.9})\n",
    "#                     | StrOutputParser()  \n",
    "#                     | createQuestionsList)\n",
    "\n",
    "# OR \n",
    "\n",
    "getMoreQuestions = ({\"context\": retriever_SD | format_docs, \"question\": RunnablePassthrough()} \n",
    "                    | prompt_rag_fusion \n",
    "                    | ChatGroq(model=\"mixtral-8x7b-32768\")  \n",
    "                    | StrOutputParser()  \n",
    "                    | createQuestionsList)\n",
    "\n",
    "#To Test\n",
    "# Question = \"Enter your Question here\"\n",
    "# getMoreQuestions.invoke(Question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain_rag_fusion = getMoreQuestions | ensemble_retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "#To Test\n",
    "# Question = \"Enter your Question here\"\n",
    "# docs = retrieval_chain_rag_fusion.invoke(Question)\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Imagine you are a developer and give me answer to the question based on the context alone.\n",
    "Since infomation within the context might be not be common knowledge and be known to you, I have included another supporting document for you to understand the context even better:\n",
    "\n",
    "And, be very direct with your answers. Include futher infomation if needed \n",
    "\n",
    "question : {question}\n",
    "\n",
    "supporting document for you to make better sense of the question : {supporting_document}\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion | format_Tdocs, \n",
    "     \"supporting_document\": retriever_SD | format_docs,\n",
    "     \"question\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Enter your Question here\"\n",
    "print(final_rag_chain.invoke(Question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V4 : Straight Rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "Imagine you are a developer and give me answer to the question based on the context alone.\n",
    "Since infomation within the context might be not be common knowledge and be known to you, I have included another supporting document for you to understand the context even better:\n",
    "\n",
    "And, be very direct with your answers. Include futher infomation if needed \n",
    "\n",
    "question : {question}\n",
    "\n",
    "supporting document: {supporting_document}\n",
    "\n",
    "context: {context}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"supporting_document\": retriever_SD | format_docs , \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question = \"Enter your Question here\"\n",
    "rag_chain.invoke(Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V5 : Straight Rag with Single Query Contruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting and Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''\n",
    "Imagine you are a developer and give me answer to the question based on the context alone.\n",
    "And, be very direct with your answers. \n",
    "\n",
    "And make sure that you do not hallucinate and answer based on the context given below only. If you are unable to answer any part of the answer, say \"I don't know\" to that part of the answer.\n",
    "\n",
    "question : {question}\n",
    "\n",
    "context: {context}\n",
    "'''\n",
    "promptAnswer = PromptTemplate.from_template(template)\n",
    "\n",
    "template2 = '''\n",
    "\n",
    "You are requested to provide the only answer and nothing else! Make it consise. The question is as follows: \n",
    "\n",
    "Imagine you are a developer and you want to prompt a LLM to answer a question for you. But the question has a lot of domain specific infomation which the LLM might not understand. \n",
    "So you creating a better question, based on your original question and the documentation that you have wrote during your coding process which explains these domain specific infomation.\n",
    "You have to give me an question which has the complete context of the documentation along the core question so that your LLM can find everything effectively.\n",
    "\n",
    "Note: Within the question, you can't say note the documentation as the llm doesn't have access to it, You should pull the relevent info from the documentation to provide me the answers. \n",
    "\n",
    "Give few short examples as well within the questions and make question lenghty but not out of context.\n",
    "\n",
    "question : {question}\n",
    "\n",
    "documentation: {documentation}\n",
    "'''\n",
    "promptQuestion = PromptTemplate.from_template(template2)\n",
    "\n",
    "Qrag_chain = (\n",
    "    {\"documentation\": retriever_SD | format_docs , \"question\": RunnablePassthrough()}\n",
    "    | promptQuestion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "Arag_chain = (\n",
    "    {\"context\": retriever | format_docs,  \"question\": RunnablePassthrough()}\n",
    "    | promptAnswer\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunModel(query):\n",
    "    Question = Qrag_chain.invoke(query)\n",
    "    return Arag_chain.invoke(Question)\n",
    "\n",
    "Question = \"Enter your Question here\"\n",
    "print(RunModel(Question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Line Chunking for Self Query Retiever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ") model_name='BAAI/bge-small-en' cache_folder=None model_kwargs={'device': 'cpu'} encode_kwargs={'normalize_embeddings': True} query_instruction='Represent this question for searching relevant passages: ' embed_instruction='' show_progress=True\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs, show_progress=True\n",
    ")\n",
    "print(hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read and split! (RD)\n",
      "Directory created: c:\\Users\\yaesh\\OneDrive\\Desktop\\College\\Projects\\RAGSync\\Data\\TCHROMA_data  (RD)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9853de1a640b48e1aea60993ef0213a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process done for RawData\n",
      "Files read and split! (SD)\n",
      "Directory created: c:\\Users\\yaesh\\OneDrive\\Desktop\\College\\Projects\\RAGSync\\Data\\TCHROMA_SDdata  (SD)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558a6818cb5942d8ae3aeb7e12ba2127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process done for SupportingDoc\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Meant for single line chunking and retriever, using bge-small-en. Not v1.5, just normal\n",
    "\"\"\"\n",
    "#The below is to read the documents from RawData and spliting them into chunks.\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from collections import Counter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import langchain_core\n",
    "\n",
    "#The below is the helper functions\n",
    "def extract_timestamp_and_tags(log_entry):\n",
    "    timestamp_pattern = r'^(\\w+ \\d{2} \\d{2}:\\d{2}:\\d{2})'\n",
    "    tags_pattern = r'\\[(.*?)\\]'\n",
    "    key_state_pattern = r'-> (\\w+)'\n",
    "\n",
    "    # Find the timestamp\n",
    "    match = re.search(timestamp_pattern, log_entry)\n",
    "    if match:\n",
    "        timestamp = match.group(1)\n",
    "        \n",
    "        # Find the tags\n",
    "        tags_match = re.findall(tags_pattern, log_entry)\n",
    "        \n",
    "        # Assign special variables\n",
    "        first_tag = tags_match.pop(0) if len(tags_match) > 0 else ''\n",
    "        thid = tags_match[0].strip('[ThId: ]') if len(tags_match) > 1 else ''\n",
    "        third_tag = tags_match[1].strip() if len(tags_match) > 2 else ''\n",
    "        \n",
    "        # Check for key state\n",
    "        key_state_match = re.search(key_state_pattern, log_entry)\n",
    "        key_state = key_state_match.group(1) if key_state_match else ''\n",
    "        \n",
    "        # Join remaining tags with spaces\n",
    "        other_tags = ' '.join(tags_match[2:]) if len(tags_match) >= 3 else ''\n",
    "        \n",
    "        return timestamp, first_tag, third_tag, other_tags, key_state, thid\n",
    "    \n",
    "    return None, None, None, None, None, None\n",
    "\n",
    "def extract_timestamps_and_tags_from_log_string(log_string):\n",
    "    log_entries = log_string.strip().split('\\n')\n",
    "    \n",
    "    entries = []\n",
    "    \n",
    "    for entry in log_entries:\n",
    "        timestamp, first_tag, third_tag, other_tags, key_state, thid = extract_timestamp_and_tags(entry)\n",
    "        if timestamp and first_tag and third_tag:\n",
    "            entries.append((timestamp, first_tag, third_tag, other_tags, key_state, thid))\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def include_MetaData(splits):\n",
    "    for i in range(len(splits)):\n",
    "        entries = extract_timestamps_and_tags_from_log_string(splits[i].page_content)\n",
    "        time_stamp, logtypes, maintags, other_tags, key_states, thids = \"None\" ,\"None\" ,\"None\" ,\"None\" ,\"None\", \"None\" \n",
    "        \n",
    "        # Combine all timestamps into one list\n",
    "        if (len(entries) > 0 and len(entries[0]) >= 6): \n",
    "            time_stamp = entries[0][0]\n",
    "            logtypes = entries[0][1]\n",
    "            maintags = entries[0][2]\n",
    "            other_tags = entries[0][3]\n",
    "            key_states = entries[0][4]\n",
    "            thids = entries[0][5]\n",
    "        \n",
    "        splits[i].metadata[\"Time_Stamp\"] = time_stamp\n",
    "        splits[i].metadata[\"Log_Type_Tag\"] = logtypes\n",
    "        splits[i].metadata[\"Main_Tag\"] = maintags\n",
    "        splits[i].metadata[\"Other_Tags\"] = other_tags\n",
    "        splits[i].metadata[\"Key_States\"] = key_states\n",
    "        splits[i].metadata['THID'] = thids\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"Data\\RawData\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents = loader.load()\n",
    "lines = re.split(r'\\r?\\n', documents[0].page_content)\n",
    "splits_lines = []\n",
    "for i in lines: \n",
    "    splits_lines.append(langchain_core.documents.base.Document(page_content = i))\n",
    "include_MetaData(splits_lines)\n",
    "print(\"Files read and split! (RD)\")\n",
    "\n",
    "# The below is to create the folder for the vector database for RawData\n",
    "# In the case of the folder already existing, it will delete the create an new folder.\n",
    "CHROMA_PATH = os.path.join(os.getcwd(), \"Data\\TCHROMA_data\")\n",
    "if not os.path.exists(CHROMA_PATH):\n",
    "    os.makedirs(CHROMA_PATH, exist_ok=True) #Creates an new folder\n",
    "else:\n",
    "    # Delete all files and folders in FAISS_PATH\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "    os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "print(f\"Directory created: {CHROMA_PATH}  (RD)\")\n",
    "\n",
    "#The below is the create the vector database for RawData. This will take a while, depending on your system's specs.\n",
    "#The below is to create the database from scratch.\n",
    "vectorstore = Chroma.from_documents(documents=splits_lines, embedding=hf_embeddings, persist_directory=CHROMA_PATH)\n",
    "print(\"Process done for RawData\")\n",
    "\n",
    "\n",
    "#The below does the same as above, but for the supporting documents. \n",
    "SDloader = DirectoryLoader(\n",
    "    \"Data\\SupportingDoc\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "SDdocuments = SDloader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "SDsplits = text_splitter.split_documents(SDdocuments)\n",
    "print(\"Files read and split! (SD)\")\n",
    "\n",
    "CHROMA_SDPATH = os.path.join(os.getcwd(), \"Data\\TCHROMA_SDdata\")\n",
    "if not os.path.exists(CHROMA_SDPATH):\n",
    "    os.makedirs(CHROMA_SDPATH, exist_ok=True) #Creates an new folder\n",
    "else:\n",
    "    # Delete all files and folders in FAISS_PATH\n",
    "    shutil.rmtree(CHROMA_SDPATH)\n",
    "    os.makedirs(CHROMA_SDPATH, exist_ok=True)\n",
    "print(f\"Directory created: {CHROMA_SDPATH}  (SD)\")\n",
    "\n",
    "# #The below is the create the vector database for RawData. This will take a while, depending on your system's specs.\n",
    "# #The below is to create the database from scratch.\n",
    "vectorstore_SD = Chroma.from_documents(documents=SDsplits, embedding=hf_embeddings, persist_directory=CHROMA_SDPATH)\n",
    "print(\"Process done for SupportingDoc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
